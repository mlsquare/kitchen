{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Following pytorch model has one linear layer with `input`: 3 and `output`:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    class network(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(network, self).__init__()\n",
    "            self.fc1 = nn.Linear(3, 1)#100)\n",
    "            #self.fc2 = nn.Linear(100, 100)\n",
    "            #self.fc3 = nn.Linear(100, 1, bias=False)\n",
    "            #self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        def forward(self, X):\n",
    "            X = F.relu(self.fc1(X))\n",
    "            #X = self.fc2(X)\n",
    "            #X = self.fc3(X)\n",
    "            #X = self.softmax(X)\n",
    "            return X\n",
    "    model= network()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    criterion= nn.MSELoss()# CrossEntropyLoss()\n",
    "    return model, optimizer, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "network(\n",
       "  (fc1): Linear(in_features=3, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2, optimizer2, loss2= create_model()\n",
    "model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Data preparation scheme:\n",
    "\n",
    "        - Each record `X` cosist of first 3 values of 4 values\n",
    "          from iris dataset input features.\n",
    "        \n",
    "        - 4th value serve as taget label `y` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(arr):\n",
    "    f= lambda x: (x-np.mean(x))/np.var(x)\n",
    "    for idx in range(arr.shape[1]):\n",
    "        arr[:,idx]= f(arr[:, idx])\n",
    "    #return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  loss:  1.8610870838165283\n",
      "epoch:  1  loss:  1.8587082624435425\n",
      "epoch:  2  loss:  1.8563390970230103\n",
      "epoch:  3  loss:  1.8539788722991943\n",
      "epoch:  4  loss:  1.8516281843185425\n",
      "epoch:  5  loss:  1.849287748336792\n",
      "epoch:  6  loss:  1.8469569683074951\n",
      "epoch:  7  loss:  1.8446362018585205\n",
      "epoch:  8  loss:  1.8423256874084473\n",
      "epoch:  9  loss:  1.8400251865386963\n",
      "epoch:  10  loss:  1.837735652923584\n",
      "epoch:  11  loss:  1.835456132888794\n",
      "epoch:  12  loss:  1.8331878185272217\n",
      "epoch:  13  loss:  1.8309293985366821\n",
      "epoch:  14  loss:  1.8286824226379395\n",
      "epoch:  15  loss:  1.826446294784546\n",
      "epoch:  16  loss:  1.8242213726043701\n",
      "epoch:  17  loss:  1.8220078945159912\n",
      "epoch:  18  loss:  1.8198052644729614\n",
      "epoch:  19  loss:  1.8176140785217285\n",
      "epoch:  20  loss:  1.815434217453003\n",
      "epoch:  21  loss:  1.8132655620574951\n",
      "epoch:  22  loss:  1.811109185218811\n",
      "epoch:  23  loss:  1.808963418006897\n",
      "epoch:  24  loss:  1.8068289756774902\n",
      "epoch:  25  loss:  1.8047070503234863\n",
      "epoch:  26  loss:  1.8025941848754883\n",
      "epoch:  27  loss:  1.8004921674728394\n",
      "epoch:  28  loss:  1.7984020709991455\n",
      "epoch:  29  loss:  1.796323299407959\n",
      "epoch:  30  loss:  1.7942556142807007\n",
      "epoch:  31  loss:  1.7922003269195557\n",
      "epoch:  32  loss:  1.7901562452316284\n",
      "epoch:  33  loss:  1.7881242036819458\n",
      "epoch:  34  loss:  1.786101222038269\n",
      "epoch:  35  loss:  1.7840855121612549\n",
      "epoch:  36  loss:  1.7820814847946167\n",
      "epoch:  37  loss:  1.7800885438919067\n",
      "epoch:  38  loss:  1.778106689453125\n",
      "epoch:  39  loss:  1.7761372327804565\n",
      "epoch:  40  loss:  1.7741782665252686\n",
      "epoch:  41  loss:  1.7722314596176147\n",
      "epoch:  42  loss:  1.7702958583831787\n",
      "epoch:  43  loss:  1.7683721780776978\n",
      "epoch:  44  loss:  1.766459584236145\n",
      "epoch:  45  loss:  1.7645583152770996\n",
      "epoch:  46  loss:  1.762669324874878\n",
      "epoch:  47  loss:  1.7607910633087158\n",
      "epoch:  48  loss:  1.7589243650436401\n",
      "epoch:  49  loss:  1.7570637464523315\n",
      "epoch:  50  loss:  1.755183458328247\n",
      "epoch:  51  loss:  1.7533111572265625\n",
      "epoch:  52  loss:  1.7514469623565674\n",
      "epoch:  53  loss:  1.7495911121368408\n",
      "epoch:  54  loss:  1.747744083404541\n",
      "epoch:  55  loss:  1.7459055185317993\n",
      "epoch:  56  loss:  1.7440764904022217\n",
      "epoch:  57  loss:  1.7422552108764648\n",
      "epoch:  58  loss:  1.7404396533966064\n",
      "epoch:  59  loss:  1.7386332750320435\n",
      "epoch:  60  loss:  1.7368359565734863\n",
      "epoch:  61  loss:  1.7350472211837769\n",
      "epoch:  62  loss:  1.7332677841186523\n",
      "epoch:  63  loss:  1.7314982414245605\n",
      "epoch:  64  loss:  1.7297141551971436\n",
      "epoch:  65  loss:  1.7279194593429565\n",
      "epoch:  66  loss:  1.7261230945587158\n",
      "epoch:  67  loss:  1.7243300676345825\n",
      "epoch:  68  loss:  1.7225427627563477\n",
      "epoch:  69  loss:  1.7207612991333008\n",
      "epoch:  70  loss:  1.7189868688583374\n",
      "epoch:  71  loss:  1.7172186374664307\n",
      "epoch:  72  loss:  1.7154523134231567\n",
      "epoch:  73  loss:  1.7136913537979126\n",
      "epoch:  74  loss:  1.7119373083114624\n",
      "epoch:  75  loss:  1.7101901769638062\n",
      "epoch:  76  loss:  1.7084503173828125\n",
      "epoch:  77  loss:  1.7067177295684814\n",
      "epoch:  78  loss:  1.7049931287765503\n",
      "epoch:  79  loss:  1.70327627658844\n",
      "epoch:  80  loss:  1.7015669345855713\n",
      "epoch:  81  loss:  1.6998660564422607\n",
      "epoch:  82  loss:  1.69817316532135\n",
      "epoch:  83  loss:  1.6964882612228394\n",
      "epoch:  84  loss:  1.6948115825653076\n",
      "epoch:  85  loss:  1.6931431293487549\n",
      "epoch:  86  loss:  1.6914829015731812\n",
      "epoch:  87  loss:  1.689831018447876\n",
      "epoch:  88  loss:  1.688187599182129\n",
      "epoch:  89  loss:  1.6865525245666504\n",
      "epoch:  90  loss:  1.6849254369735718\n",
      "epoch:  91  loss:  1.6833066940307617\n",
      "epoch:  92  loss:  1.6816962957382202\n",
      "epoch:  93  loss:  1.680091142654419\n",
      "epoch:  94  loss:  1.6784886121749878\n",
      "epoch:  95  loss:  1.6768933534622192\n",
      "epoch:  96  loss:  1.6753050088882446\n",
      "epoch:  97  loss:  1.6737236976623535\n",
      "epoch:  98  loss:  1.6721501350402832\n",
      "epoch:  99  loss:  1.6705838441848755\n",
      "epoch:  100  loss:  1.66902494430542\n",
      "epoch:  101  loss:  1.6674731969833374\n",
      "epoch:  102  loss:  1.6659289598464966\n",
      "epoch:  103  loss:  1.6643924713134766\n",
      "epoch:  104  loss:  1.662862777709961\n",
      "epoch:  105  loss:  1.6613409519195557\n",
      "epoch:  106  loss:  1.6598261594772339\n",
      "epoch:  107  loss:  1.658319354057312\n",
      "epoch:  108  loss:  1.656819224357605\n",
      "epoch:  109  loss:  1.6553168296813965\n",
      "epoch:  110  loss:  1.6538165807724\n",
      "epoch:  111  loss:  1.6523141860961914\n",
      "epoch:  112  loss:  1.650734305381775\n",
      "epoch:  113  loss:  1.6491531133651733\n",
      "epoch:  114  loss:  1.6475709676742554\n",
      "epoch:  115  loss:  1.6459890604019165\n",
      "epoch:  116  loss:  1.6444087028503418\n",
      "epoch:  117  loss:  1.6428298950195312\n",
      "epoch:  118  loss:  1.6412537097930908\n",
      "epoch:  119  loss:  1.6396805047988892\n",
      "epoch:  120  loss:  1.6381113529205322\n",
      "epoch:  121  loss:  1.6365466117858887\n",
      "epoch:  122  loss:  1.6349866390228271\n",
      "epoch:  123  loss:  1.6334319114685059\n",
      "epoch:  124  loss:  1.6318154335021973\n",
      "epoch:  125  loss:  1.6301745176315308\n",
      "epoch:  126  loss:  1.6285313367843628\n",
      "epoch:  127  loss:  1.6268666982650757\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "X = data.data#, data.target\n",
    "scale(X)\n",
    "\n",
    "trainx= Variable(torch.Tensor(X[:,:-1]).float(), requires_grad=True) #df.iloc[:, :-1].values).float()\n",
    "trainy = Variable(torch.reshape(torch.Tensor(X[:,-1]).float(), (-1,1)))\n",
    "\n",
    "#i_grads= np.empty(trainx.shape)\n",
    "all_grads= list()\n",
    "for epoch in range(128):\n",
    "    optimizer2.zero_grad()\n",
    "    y_pred = model2(trainx)\n",
    "    #print(y_pred)\n",
    "    l = loss2(y_pred, trainy)\n",
    "    print('epoch: ', epoch, ' loss: ', l.item())\n",
    "    # perform a backward pass (backpropagation)\n",
    "    l.backward()\n",
    "    wts_bias = list(model2.parameters())\n",
    "    wts_bias_grads = torch.cat((wts_bias[0].grad, torch.reshape(wts_bias[1].grad, (-1,1))),1)\n",
    "    #i_grads= trainx.grad\n",
    "    all_grads.append(wts_bias_grads.tolist())#i_grads)\n",
    "    # Update the parameters\n",
    "    optimizer2.step()\n",
    "all_grads = np.array(all_grads).reshape(-1,4)\n",
    "all_grads_df= pd.DataFrame(all_grads, columns=['beta_{}'.format(idx) for idx in range(1,5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Following are the weights and bias for the ONLY layer in model, after 128 epochs of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.3514, -0.4006,  0.1464]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.2750], requires_grad=True)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li = list(model2.parameters())\n",
    "li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final weights gradients : tensor([[-0.5502, -0.9713, -0.2204]])\n",
      "Final bias gradients: tensor([-0.1030])\n"
     ]
    }
   ],
   "source": [
    "print('Final weights gradients : {}\\nFinal bias gradients: {}'.format(li[0].grad, li[1].grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Following are gradients updated after each epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beta_1</th>\n",
       "      <th>beta_2</th>\n",
       "      <th>beta_3</th>\n",
       "      <th>beta_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-0.569870</td>\n",
       "      <td>-1.490341</td>\n",
       "      <td>-0.170971</td>\n",
       "      <td>0.152157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.568760</td>\n",
       "      <td>-1.484230</td>\n",
       "      <td>-0.171056</td>\n",
       "      <td>0.150241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.567649</td>\n",
       "      <td>-1.478120</td>\n",
       "      <td>-0.171140</td>\n",
       "      <td>0.148325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-0.566538</td>\n",
       "      <td>-1.472011</td>\n",
       "      <td>-0.171224</td>\n",
       "      <td>0.146410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-0.565427</td>\n",
       "      <td>-1.465905</td>\n",
       "      <td>-0.171308</td>\n",
       "      <td>0.144496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>-0.512670</td>\n",
       "      <td>-0.958388</td>\n",
       "      <td>-0.197748</td>\n",
       "      <td>-0.065333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>-0.546060</td>\n",
       "      <td>-0.975163</td>\n",
       "      <td>-0.214689</td>\n",
       "      <td>-0.090420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>-0.543684</td>\n",
       "      <td>-0.972776</td>\n",
       "      <td>-0.214009</td>\n",
       "      <td>-0.090020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>-0.541277</td>\n",
       "      <td>-0.970449</td>\n",
       "      <td>-0.213312</td>\n",
       "      <td>-0.089577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>-0.550168</td>\n",
       "      <td>-0.971309</td>\n",
       "      <td>-0.220399</td>\n",
       "      <td>-0.102953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       beta_1    beta_2    beta_3    beta_4\n",
       "0   -0.569870 -1.490341 -0.170971  0.152157\n",
       "1   -0.568760 -1.484230 -0.171056  0.150241\n",
       "2   -0.567649 -1.478120 -0.171140  0.148325\n",
       "3   -0.566538 -1.472011 -0.171224  0.146410\n",
       "4   -0.565427 -1.465905 -0.171308  0.144496\n",
       "..        ...       ...       ...       ...\n",
       "123 -0.512670 -0.958388 -0.197748 -0.065333\n",
       "124 -0.546060 -0.975163 -0.214689 -0.090420\n",
       "125 -0.543684 -0.972776 -0.214009 -0.090020\n",
       "126 -0.541277 -0.970449 -0.213312 -0.089577\n",
       "127 -0.550168 -0.971309 -0.220399 -0.102953\n",
       "\n",
       "[128 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_grads_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Following computes the values at gradients w.r.t all inputs `X`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([150, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer2.zero_grad()\n",
    "y_pred = model2(trainx)\n",
    "l = loss2(y_pred, trainy)\n",
    "l.backward()\n",
    "i_grads= trainx.grad\n",
    "#all_grads.append(i_grads)\n",
    "#grad w.r.t input dataset Could be computed for every epoch\n",
    "\n",
    "i_grads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beta_1</th>\n",
       "      <th>beta_2</th>\n",
       "      <th>beta_3</th>\n",
       "      <th>beta_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>-0.534177</td>\n",
       "      <td>-1.171829</td>\n",
       "      <td>-0.184331</td>\n",
       "      <td>0.034866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>0.017789</td>\n",
       "      <td>0.161002</td>\n",
       "      <td>0.012604</td>\n",
       "      <td>0.071170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>-0.569870</td>\n",
       "      <td>-1.490341</td>\n",
       "      <td>-0.220399</td>\n",
       "      <td>-0.102953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>-0.548793</td>\n",
       "      <td>-1.301782</td>\n",
       "      <td>-0.193073</td>\n",
       "      <td>-0.027634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>-0.534617</td>\n",
       "      <td>-1.147826</td>\n",
       "      <td>-0.185654</td>\n",
       "      <td>0.029436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>-0.520930</td>\n",
       "      <td>-1.020378</td>\n",
       "      <td>-0.171876</td>\n",
       "      <td>0.103084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>-0.495673</td>\n",
       "      <td>-0.958388</td>\n",
       "      <td>-0.166877</td>\n",
       "      <td>0.152157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           beta_1      beta_2      beta_3      beta_4\n",
       "count  128.000000  128.000000  128.000000  128.000000\n",
       "mean    -0.534177   -1.171829   -0.184331    0.034866\n",
       "std      0.017789    0.161002    0.012604    0.071170\n",
       "min     -0.569870   -1.490341   -0.220399   -0.102953\n",
       "25%     -0.548793   -1.301782   -0.193073   -0.027634\n",
       "50%     -0.534617   -1.147826   -0.185654    0.029436\n",
       "75%     -0.520930   -1.020378   -0.171876    0.103084\n",
       "max     -0.495673   -0.958388   -0.166877    0.152157"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_grads_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Mean values for weight(Beta_1-3) and bias(beta_4) gradients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "beta_1   -0.534177\n",
       "beta_2   -1.171829\n",
       "beta_3   -0.184331\n",
       "beta_4    0.034866\n",
       "Name: mean, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_grads_df.describe().iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Evaluating same data with linear regression from statsmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "feats, targets= X[:,:-1], X[:,-1]\n",
    "feats= sm.add_constant(feats, prepend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.938\n",
      "Model:                            OLS   Adj. R-squared:                  0.937\n",
      "Method:                 Least Squares   F-statistic:                     734.4\n",
      "Date:                Thu, 12 Mar 2020   Prob (F-statistic):           7.83e-88\n",
      "Time:                        18:00:03   Log-Likelihood:                -45.701\n",
      "No. Observations:                 150   AIC:                             99.40\n",
      "Df Residuals:                     146   BIC:                             111.4\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1            -0.2446      0.056     -4.363      0.000      -0.355      -0.134\n",
      "x2             0.0729      0.016      4.553      0.000       0.041       0.104\n",
      "x3             2.8110      0.131     21.399      0.000       2.551       3.071\n",
      "const       9.368e-17      0.027   3.45e-15      1.000      -0.054       0.054\n",
      "==============================================================================\n",
      "Omnibus:                        5.609   Durbin-Watson:                   1.573\n",
      "Prob(Omnibus):                  0.061   Jarque-Bera (JB):                6.811\n",
      "Skew:                           0.223   Prob(JB):                       0.0332\n",
      "Kurtosis:                       3.944   Cond. No.                         12.1\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "model = sm.OLS(targets, feats, hasconst=True)\n",
    "result= model.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Weights and biases from modelstats regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.44611824e-01,  7.28612405e-02,  2.81096557e+00,  9.36750677e-17])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def model():\n",
    "#    class network(nn.Module):\n",
    "#        def __init__(self):\n",
    "#            super(network, self).__init__()\n",
    "#            self.fc1 = nn.Linear(4, 100)\n",
    "#            self.fc2 = nn.Linear(100, 100)\n",
    "#            self.fc3 = nn.Linear(100, 3)\n",
    "#            self.softmax = nn.Softmax(dim=1)\n",
    "#\n",
    "#        def forward(self, X):\n",
    "#            X = F.relu(self.fc1(X))\n",
    "#           X = self.fc2(X)\n",
    "#            X = self.fc3(X)\n",
    "            X = self.softmax(X)\n",
    "#            return X\n",
    "#    model= network()\n",
    "#    optimizer = optim.Adam(model.parameters())\n",
    "#    criterion= nn.CrossEntropyLoss()\n",
    "#    return model, optimizer, criterion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
